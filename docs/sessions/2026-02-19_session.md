# Session: 2026-02-19

**Status:** Complete

## What Was Accomplished

### Phase 1: Pipeline Audit & Emergency Fixes (commit 02c37da)

**Problem discovered:** The weekly data collection pipeline was silently broken. The YouTube API key had expired since at least Feb 2. Every YouTube collector failed with "API key expired" but the CI workflow reported success because all collectors were wrapped in `|| true`. Empty CSVs (0 rows) were being written and picked up as "latest" by the score calculator, degrading all 8 metric scores.

**Fixes applied:**
- Added empty-data guards to 6 YouTube collectors (ai-psychosis, wage-stagnation, housing-despair, dating-app-despair, layoff-watch, airline-chaos). The healthcare and subscription-overload collectors already had guards.
- Added empty-data guards to all 8 Reddit collectors
- Made `calculate_all_social_scores.py` skip CSV files with fewer than 5 data rows via new `count_data_rows()` function and updated `find_latest_file()`
- Added validation gate in CI workflow (`check_data` step) that checks if any new CSVs with real data (>200 bytes) were produced. All downstream steps (dedup, score calc, TS update, build, commit) are now conditional on `has_new_data == 'true'`
- Changed CI `pip install` to use `requirements.txt` instead of inline dependencies
- Fixed commit message from "YouTube and Reddit" to "YouTube and TikTok" (Reddit doesn't run in CI)
- Fixed typo in airline-chaos collector: `'rebookedmultiple times'` -> `'rebooked multiple times'`
- Created `data-collection/requirements.txt` and `data-collection/.env.example`
- Updated `.gitignore` with exceptions for `.env.example` files

**API key renewed:** User added new YouTube API key to both GitHub secrets and Vercel. Triggered manual CI run (22197837252) - confirmed fresh data collected for all 8 metrics.

### Phase 2: Frontend & Accessibility Improvements (commit 24dcd8e)

- **WCAG contrast:** Changed `bg-red-600` to `bg-red-700` where it serves as background for white text (crisis cards, score breakdown, methodology sections). Passes AA at 4.68:1. Left `text-red-600` on dark backgrounds unchanged (5.4:1, already passes). Left decorative uses (progress bars, borders) unchanged.
- **Emoji removal:** Replaced emoji characters in MetricDetail.tsx (`viewCount`/`commentCount` display) with text labels. Design system says no emojis.
- **Accessibility:**
  - Added `aria-label` to metric cards describing score and status
  - Added Space key activation alongside existing Enter key on cards
  - Added focus trap to MetricDetail modal (Tab cycles within modal)
  - Added `role="dialog"`, `aria-modal="true"`, `aria-label` to modal
  - Auto-focuses close button when modal opens
  - Added skip-to-content link on main page
- **Font optimization:** Replaced CSS `@import url()` with `next/font/google` imports in layout.tsx. Fonts now self-hosted and loaded without render blocking. CSS variables (`--font-archivo`, `--font-space`, `--font-mono`) wired through globals.css.
- **Dynamic methodology page:** Rewrote `methodology/page.tsx` to import from `metricDetailData.ts`. All sample counts, platform stats, and the summary table now auto-update when scores change. Footer shows live update date and total data points.
- **Trend calculation:** Added `read_current_scores()` and `calculate_trend()` to `update_metric_data.py`. Before overwriting scores, it reads the current values from the TS file, compares old vs new, and writes `"worsening"` / `"improving"` / `"neutral"` based on a 2-point threshold. No more hardcoded "worsening" on every metric.
- **Status page:** Created `/status` route showing pipeline health: total data points, metrics tracked, last update date, per-platform collection health (progress bars), and per-metric status with freshness indicators (green/yellow/red based on days since update). Added STATUS nav link in header.

### Phase 3: Project Infrastructure

- Created `docs/sessions/` directory with README and this session log
- Created `CLAUDE.md` with project conventions and key file index

### Phase 4: Deep Audit & Path Fixes (commit 48a4097)

**Problem discovered:** 4 YouTube collectors (wage-stagnation, housing-despair, dating-app-despair, layoff-watch) were writing output to `data-collection/collected-data/` instead of `collected-data/`. Since CI runs with `working-directory: data-collection/`, this created a nested `data-collection/data-collection/collected-data/` directory that was never committed. These 4 metrics had stale YouTube data since January 6 (44 days).

**Fix:** Changed all 4 collectors to use `collected-data/` matching the other working collectors.

### Phase 5: Centralized Configuration (commit pending)

- Created `data-collection/config.json` as single source of truth for:
  - Formula weights (official=0.4, social=0.6)
  - All 6 severity weights (LEVEL_1 through LEVEL_3)
  - All 8 metric definitions (name, slug, official_score, patterns, collection_targets)
- Updated `calculate_all_social_scores.py` and `update_metric_data.py` to load from config.json
- Eliminates duplication of official scores across 3+ files and formula weights across 10 locations

### Phase 6: New Data Sources (commit pending)

Built 4 new data collectors to expand coverage from 3 platforms (YouTube, Reddit, TikTok) to 7:

1. **FRED API collector** (`fred-collector.py`): Pulls live economic data from Federal Reserve. Computes official scores for 4 metrics (wage stagnation via real wage growth, housing despair via price-to-income ratio, layoff watch via initial claims, healthcare via consumer sentiment). Falls back to config.json for metrics without FRED series (AI psychosis, subscriptions, dating, airlines). Outputs `collected-data/official_scores.json`.

2. **Hacker News collector** (`hackernews-collector.py`): Uses free Algolia API (no auth needed). Covers 5 metrics with 23 search terms. Tested locally: 486 stories collected across layoff_watch (122), ai_psychosis (120), subscription_overload (117), housing_despair (97), wage_stagnation (30).

3. **CFPB complaint collector** (`cfpb-collector.py`): Uses free Consumer Complaint Database API (no auth needed). Covers healthcare (medical debt), housing_despair (mortgage complaints), and subscription_overload (billing disputes). Tested locally: 200 complaints collected.

4. **Bluesky collector** (`bluesky-collector.py`): Uses public AT Protocol API (no auth needed). Covers all 8 metrics with per-metric severity keywords. Note: Bluesky API currently returning 403 from CDN; collector handles gracefully with empty-data guards.

### Phase 7: Pipeline Integration (commit pending)

- Updated `calculate_all_social_scores.py` to discover and include HN, CFPB, Bluesky CSVs alongside YouTube/Reddit/TikTok. Added FRED official scores to replace hardcoded values when available. Added `points` and `like_count` to engagement field search.
- Updated `update_metric_data.py` with:
  - `get_engagement_value()` helper for cross-source engagement extraction
  - `get_source_data()` generic CSV reader replacing per-source boilerplate
  - `load_fred_scores()` for live official score integration
  - `get_latest_file()` now skips files with <5 rows (was picking empty Feb 2 files)
  - Refactored `calculate_metric_scores()` to process all 6 sources in a single loop
  - Added dataSources regex updaters for HN, CFPB, Bluesky
- Updated CI workflow (`.github/workflows/weekly-data-update.yml`):
  - Added FRED_API_KEY environment variable
  - Added 4 new collector steps (FRED, HN, CFPB, Bluesky)
  - Updated data validation to include .json files
  - Updated commit message to reflect all data sources
- Updated `requirements.txt` with `fredapi>=0.5.0`
- Updated `.env.example` with `FRED_API_KEY`

**Result:** Total data entries across all metrics increased from ~2,000 to 3,101. Both scoring scripts now produce identical results.

## Files Changed

### Commit 1: `02c37da` - Harden data collection pipeline
- `data-collection/ai-psychosis-youtube-collector.py`
- `data-collection/wage-stagnation-youtube-collector.py`
- `data-collection/housing-despair-youtube-collector.py`
- `data-collection/dating-app-despair-youtube-collector.py`
- `data-collection/layoff-watch-youtube-collector.py`
- `data-collection/airline-chaos-youtube-collector.py`
- `data-collection/healthcare-reddit-collector.py`
- `data-collection/ai-psychosis-reddit-collector.py`
- `data-collection/subscription-overload-reddit-collector.py`
- `data-collection/wage-stagnation-reddit-collector.py`
- `data-collection/housing-despair-reddit-collector.py`
- `data-collection/dating-app-despair-reddit-collector.py`
- `data-collection/layoff-watch-reddit-collector.py`
- `data-collection/airline-chaos-reddit-collector.py`
- `data-collection/calculate_all_social_scores.py`
- `data-collection/requirements.txt` (new)
- `data-collection/.env.example` (new)
- `.github/workflows/weekly-data-update.yml`
- `.gitignore`

### Commit 2: `24dcd8e` - Accessibility, contrast, fonts, status page
- `app/globals.css`
- `app/layout.tsx`
- `app/methodology/page.tsx`
- `app/page.tsx`
- `app/status/page.tsx` (new)
- `components/Header.tsx`
- `components/MetricCard.tsx`
- `components/MetricDetail.tsx`
- `data-collection/update_metric_data.py`

### Commit 3: `d8e522c` - Session logs and CLAUDE.md
- `docs/sessions/README.md` (new)
- `docs/sessions/2026-02-19_session.md` (new)
- `CLAUDE.md` (new)

### Commit 4: `48a4097` - Fix output paths in 4 YouTube collectors
- `data-collection/wage-stagnation-youtube-collector.py`
- `data-collection/housing-despair-youtube-collector.py`
- `data-collection/dating-app-despair-youtube-collector.py`
- `data-collection/layoff-watch-youtube-collector.py`

### Commit 5: (pending) New data sources and pipeline integration
- `data-collection/config.json` (new - centralized config)
- `data-collection/fred-collector.py` (new)
- `data-collection/hackernews-collector.py` (new)
- `data-collection/cfpb-collector.py` (new)
- `data-collection/bluesky-collector.py` (new)
- `data-collection/calculate_all_social_scores.py` (new sources + FRED)
- `data-collection/update_metric_data.py` (refactored, new sources + FRED)
- `data-collection/requirements.txt` (added fredapi)
- `data-collection/.env.example` (added FRED_API_KEY)
- `.github/workflows/weekly-data-update.yml` (new collectors + FRED env)

## Key Decisions

1. **red-700 not red-800 for contrast fix.** Red-700 (#b91c1c) gives 4.68:1 against white (passes AA) while staying visually close to the original red-600. Red-800 would be overkill and noticeably darker.
2. **Trend uses "neutral" not "stable".** The TypeScript types already had `'neutral'` as a valid trend value, and the UI renders it as "NEUTRAL". Kept consistent rather than introducing a new value.
3. **2-point threshold for trend calculation.** Prevents noise from flipping trends on tiny score fluctuations between runs.
4. **Methodology page pulls from metricDetailData, not separate data file.** Since the TS file is the single source of truth (updated weekly by CI), the methodology page reads directly from it. No duplication.
5. **Centralized config.json over per-script hardcoding.** Official scores, severity weights, and formula weights were duplicated across 3+ Python files. config.json is the single source; scripts import from it.
6. **File selection by filename timestamp, not mtime.** Changed `get_latest_file()` in `update_metric_data.py` to sort by filename (which contains `YYYYMMDD_HHMMSS`) and skip files with <5 rows. Previously used `os.path.getmtime` which could pick empty files that were touched more recently.
7. **Skipped Google Trends replacement.** `pytrends` is unreliable and the collector was never integrated into CI or scoring. With 4 new reliable sources (FRED, HN, CFPB, Bluesky), the marginal value of Google Trends is low.
8. **FRED as partial official score replacement.** 4 of 8 metrics have suitable FRED economic series. Other 4 fall back to config.json. This makes official scores dynamic for economic metrics while keeping stable baselines for cultural metrics (AI psychosis, dating, etc.).

## Next Tasks (Prioritized)

1. **Get FRED API key and add to GitHub secrets.** The FRED collector is built but needs `FRED_API_KEY` set in both `.env` (local) and GitHub repo secrets. Free at https://fred.stlouisfed.org/docs/api/api_key.html
2. **Cross-source deduplication.** YouTube and TikTok can surface the same content. No dedup exists across sources (only within-source dedup exists for HN/Bluesky).
3. **Add new sources to metricDetailData.ts display.** The dataSources arrays in the TS file don't yet include HN, CFPB, Bluesky entries. The regex updater can write them once they exist. Need to add template entries.
4. **Reddit CI workaround.** Reddit blocks GitHub Actions IPs (403). Reddit data is only collected locally. Consider: proxy, separate local trigger, or scraping alternative.
5. **Investigate Bluesky API 403.** The public search API (`app.bsky.feed.searchPosts`) returns 403 from CDN level. May need auth or alternative endpoint.
6. **Engagement unit normalization.** Reddit "score", YouTube "views", HN "points", and Bluesky "likes" are all treated equivalently in the log10 weighting, but measure different things.

## Known Issues

- **Reddit collectors blocked in CI.** All 8 Reddit collectors fail with 403 from GitHub Actions. They work locally. Reddit data comes from local Jan 6 collection.
- **Bluesky API returning 403.** The public search endpoint is blocked at CDN level. The collector is built and handles this gracefully (no empty files). Will work when API access is restored or auth is added.
- **Official scores partially automated.** FRED collector covers 4 of 8 metrics (wage stagnation, housing, layoffs, healthcare). Other 4 (AI psychosis, subscriptions, dating, airlines) still use hardcoded config.json values.
- **TikTok collector uses YouTube API.** TikTok content is collected via YouTube compilation videos, not directly from TikTok. Introduces curation bias.
- **Google Trends collector not integrated.** Exists but is not in CI and its data isn't used by the scoring pipeline. The `pytrends` library is unreliable.
